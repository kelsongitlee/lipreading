# ============================
# Part 3: Real-time Webcam Lip Reading
# ============================

import os, sys, cv2, time, torch, tempfile, threading, base64, numpy as np
from concurrent.futures import ThreadPoolExecutor
from IPython.display import display, HTML, Javascript
from google.colab import output
import warnings
warnings.filterwarnings("ignore")

# Configuration - Force GPU like Part 2
REPO_DIR = "/content/lipreading"
CONFIG_PATH = f"{REPO_DIR}/configs/LRS3_V_WER19.1.ini"
GPU_IDX = 0

# Debug and fix import paths
print(f"üìÅ Current working directory: {os.getcwd()}")
print(f"üìÅ Changing to: {REPO_DIR}")

# Change to repo directory and add to Python path
os.chdir(REPO_DIR)
sys.path.insert(0, REPO_DIR)

print(f"üìÅ Now in: {os.getcwd()}")
print(f"üîç Python path: {sys.path[:3]}...")

# Check if files exist
pipeline_file = "pipelines/pipeline.py"
detector_file = "pipelines/detectors/mediapipe/detector.py"

print(f"üîç Checking {pipeline_file}: {os.path.exists(pipeline_file)}")
print(f"üîç Checking {detector_file}: {os.path.exists(detector_file)}")

# List what's actually in the pipelines directory
if os.path.exists("pipelines"):
    print(f"üìÇ Contents of pipelines/: {os.listdir('pipelines')}")
    if os.path.exists("pipelines/detectors"):
        print(f"üìÇ Contents of pipelines/detectors/: {os.listdir('pipelines/detectors')}")
        if os.path.exists("pipelines/detectors/mediapipe"):
            print(f"üìÇ Contents of pipelines/detectors/mediapipe/: {os.listdir('pipelines/detectors/mediapipe')}")

try:
    from pipelines.pipeline import InferencePipeline
    print("‚úÖ InferencePipeline imported successfully")
except Exception as e:
    print(f"‚ùå Failed to import InferencePipeline: {e}")
    
# Install and import MediaPipe for direct face detection
try:
    import mediapipe as mp
    print("‚úÖ MediaPipe imported successfully")
except ImportError:
    print("üì¶ Installing MediaPipe...")
    import subprocess
    subprocess.run([sys.executable, "-m", "pip", "install", "mediapipe"], check=True)
    import mediapipe as mp
    print("‚úÖ MediaPipe installed and imported successfully")

class RealtimeLipReader:
    def __init__(self):
        print("üöÄ Initializing Real-time Lip Reader...")
        
        # Load model with GPU
        device = torch.device(f"cuda:{GPU_IDX}" if torch.cuda.is_available() and GPU_IDX >= 0 else "cpu")
        self.pipeline = InferencePipeline(
            config_filename=CONFIG_PATH,
            detector="mediapipe",
            face_track=True,
            device=device
        )
        
        # Face detector for live feedback - use MediaPipe directly
        self.mp_face_mesh = mp.solutions.face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        print(f"‚úÖ Using device: {device}")
        print("‚úÖ Model loaded successfully!")
        
        # Recording state
        self.recording = False
        self.executor = ThreadPoolExecutor(max_workers=1)
        
        # Video parameters for file recording (higher FPS for better accuracy)
        self.fps = 25
        self.frame_interval = 1 / self.fps
        self.min_duration = 2  # Minimum 2 seconds
        
        # Live preview state
        self.face_detected = False
        self.speaking_detected = False
        self.prev_landmarks = None
        self.landmark_history = []
        self.debug_counter = 0
        
        print("‚úÖ Real-time Lip Reader ready!")
    
    def detect_face_and_speaking(self, frame):
        """Detect if face is present and if person is speaking"""
        try:
            # Convert frame to RGB for MediaPipe (frame is already RGB from webcam)
            frame_rgb = frame
            
            # Detect face landmarks using MediaPipe
            results = self.mp_face_mesh.process(frame_rgb)
            
            # Debug output (only occasionally)
            self.debug_counter += 1
            if self.debug_counter % 30 == 1:  # Debug every 30 frames
                print(f"üîç MediaPipe results: {results.multi_face_landmarks is not None}")
            
            if results.multi_face_landmarks:
                self.face_detected = True
                
                # Get the first face landmarks
                face_landmarks = results.multi_face_landmarks[0]
                
                # Convert landmarks to numpy array
                landmarks = []
                for landmark in face_landmarks.landmark:
                    landmarks.append([landmark.x, landmark.y])
                landmarks = np.array(landmarks)
                
                # Check for mouth movement (speaking detection)
                if self.prev_landmarks is not None:
                    try:
                        # Get mouth region landmarks (indices around mouth area)
                        # MediaPipe face mesh has different indices than dlib
                        mouth_indices = list(range(0, 17)) + list(range(267, 284)) + list(range(269, 271)) + list(range(271, 272))
                        mouth_indices = [i for i in mouth_indices if i < len(landmarks)]
                        
                        if len(mouth_indices) > 5:  # Need sufficient landmarks
                            mouth_current = landmarks[mouth_indices]
                            mouth_prev = self.prev_landmarks[mouth_indices] if len(self.prev_landmarks) > max(mouth_indices) else self.prev_landmarks[-len(mouth_indices):]
                            
                            # Calculate average movement
                            movement = np.mean(np.linalg.norm(mouth_current - mouth_prev, axis=1))
                            
                            # Add to history for smoothing
                            self.landmark_history.append(movement)
                            if len(self.landmark_history) > 5:
                                self.landmark_history.pop(0)
                            
                            # Speaking detected if average movement is above threshold
                            avg_movement = np.mean(self.landmark_history)
                            self.speaking_detected = avg_movement > 0.01  # Adjusted threshold for MediaPipe coordinates
                            
                            if self.debug_counter % 30 == 1:  # Debug every 30 frames
                                print(f"üëÑ Movement: {movement:.4f}, Avg: {avg_movement:.4f}, Speaking: {self.speaking_detected}")
                        
                    except Exception as e:
                        if self.debug_counter % 30 == 1:
                            print(f"‚ùå Speaking detection error: {e}")
                        self.speaking_detected = False
                
                self.prev_landmarks = landmarks.copy()
            else:
                self.face_detected = False
                self.speaking_detected = False
                if self.debug_counter % 60 == 1:  # Debug less frequently for no face
                    print("‚ùå No face detected")
                
        except Exception as e:
            if self.debug_counter % 30 == 1:
                print(f"‚ùå Face detection error: {e}")
                import traceback
                traceback.print_exc()
            self.face_detected = False
            self.speaking_detected = False
    
    def process_video(self, video_path):
        """Process video file and return result"""
        try:
            print("üîÑ Processing video...")
            result = self.pipeline(video_path)
            print(f"üó£Ô∏è Result: {result}")
            
            # Clean up video file
            if os.path.exists(video_path):
                os.remove(video_path)
            
            return result
        except Exception as e:
            print(f"‚ùå Processing error: {e}")
            if os.path.exists(video_path):
                os.remove(video_path)
            return None
    
    def start_web_preview(self):
        """Start web-based preview with live feedback"""
        # Video recording state
        self.frames_for_video = []
        self.recording_start_time = None
        self.futures = []
        
        # Create web interface
        html_interface = """
        <div id="webcam-container">
            <canvas id="canvas" width="640" height="480" style="border: 3px solid #ccc;"></canvas>
            <div style="display: flex; justify-content: center; align-items: center; margin: 10px; gap: 20px;">
                <!-- Recording Indicator -->
                <div style="display: flex; align-items: center; gap: 5px;">
                    <div id="record-indicator" style="width: 20px; height: 20px; border-radius: 50%; background: red;"></div>
                    <span id="record-status">READY</span>
                </div>
                <!-- Face Detection -->
                <div style="display: flex; align-items: center; gap: 5px;">
                    <div id="face-indicator" style="width: 20px; height: 20px; border-radius: 50%; background: gray;"></div>
                    <span id="face-status">Waiting</span>
                </div>
                <!-- Speaking Detection -->
                <div style="display: flex; align-items: center; gap: 5px;">
                    <div id="speak-indicator" style="width: 20px; height: 20px; border-radius: 50%; background: gray;"></div>
                    <span id="speak-status">Waiting</span>
                </div>
            </div>
            <div id="instructions">Press SPACE to start/stop recording | ENTER to force process</div>
            <div id="result">Ready! Press SPACE to record, speak, then SPACE again to process</div>
        </div>
        
        <style>
        #webcam-container { text-align: center; padding: 10px; font-family: Arial; }
        #canvas { display: block; margin: 10px auto; }
        #instructions { font-weight: bold; margin: 10px; color: #333; }
        #result { margin: 10px; padding: 10px; background: #f0f0f0; border-radius: 5px; min-height: 30px; }
        </style>
        
        <script>
        let canvas = document.getElementById('canvas');
        let ctx = canvas.getContext('2d');
        let video = document.createElement('video');
        let isCapturing = false;
        
        // Initialize webcam
        navigator.mediaDevices.getUserMedia({video: {width: 640, height: 480}})
            .then(stream => {
                video.srcObject = stream;
                video.play();
                startPreview();
            })
            .catch(err => {
                document.getElementById('result').innerHTML = 'Camera error: ' + err.message;
            });
        
        function startPreview() {
            let lastFrameTime = 0;
            const frameInterval = 100; // Send frame every 100ms (10 FPS) to reduce lag
            
            function drawFrame(currentTime) {
                if (video.readyState === video.HAVE_ENOUGH_DATA) {
                    ctx.drawImage(video, 0, 0, 640, 480);
                    
                    // Only send frames at reduced rate and when recording
                    if (currentTime - lastFrameTime >= frameInterval) {
                        // Use higher quality for better accuracy
                        let imageData = canvas.toDataURL('image/jpeg', 0.95);
                        let base64Data = imageData.split(',')[1];
                        google.colab.kernel.invokeFunction('process_frame', [base64Data], {});
                        lastFrameTime = currentTime;
                    }
                }
                requestAnimationFrame(drawFrame);
            }
            requestAnimationFrame(drawFrame);
        }
        
        // Keyboard controls
        document.addEventListener('keydown', function(event) {
            if (event.code === 'Space') {
                event.preventDefault();
                google.colab.kernel.invokeFunction('toggle_recording', [], {});
            } else if (event.code === 'Enter') {
                event.preventDefault();
                google.colab.kernel.invokeFunction('force_process', [], {});
            }
        });
        
        // Update status indicators
        function updateStatus(recording, faceDetected, speaking) {
            // Recording indicator
            let recordIndicator = document.getElementById('record-indicator');
            let recordStatus = document.getElementById('record-status');
            if (recording) {
                recordIndicator.style.background = 'green';
                recordStatus.textContent = 'RECORDING';
            } else {
                recordIndicator.style.background = 'red';
                recordStatus.textContent = 'READY';
            }
            
            // Face detection indicator (only update when recording)
            let faceIndicator = document.getElementById('face-indicator');
            let faceStatus = document.getElementById('face-status');
            if (recording) {
                if (faceDetected) {
                    faceIndicator.style.background = 'blue';
                    faceStatus.textContent = 'Face Detected';
                } else {
                    faceIndicator.style.background = 'gray';
                    faceStatus.textContent = 'No Face';
                }
            } else {
                faceIndicator.style.background = 'gray';
                faceStatus.textContent = 'Waiting';
            }
            
            // Speaking indicator (only update when recording)
            let speakIndicator = document.getElementById('speak-indicator');
            let speakStatus = document.getElementById('speak-status');
            if (recording) {
                if (speaking) {
                    speakIndicator.style.background = 'orange';
                    speakStatus.textContent = 'Speaking';
                } else {
                    speakIndicator.style.background = 'gray';
                    speakStatus.textContent = 'Silent';
                }
            } else {
                speakIndicator.style.background = 'gray';
                speakStatus.textContent = 'Waiting';
            }
        }
        
        // Focus for keyboard events
        canvas.tabIndex = 1000;
        canvas.focus();
        </script>
        """
        
        display(HTML(html_interface))
        
        print("üé¨ Lip Reading Interface Ready!")
        print("üìπ Webcam preview visible above")
        print("üéÆ Keyboard Controls:")
        print("  ‚Ä¢ SPACE: Start/Stop recording")
        print("  ‚Ä¢ ENTER: Force process current buffer")
        print("üìä Status Indicators (active only when recording):")
        print("  ‚Ä¢ üî¥ READY / üü¢ RECORDING")
        print("  ‚Ä¢ üîµ Face detected (when recording)")
        print("  ‚Ä¢ üü† Speaking detected (when recording)")
        print("üìù Results appear only when recording stops")

# Global variables for JavaScript callbacks
lip_reader = None
current_video_writer = None
video_frames = []
last_record_time = 0

# JavaScript callback functions
def process_frame(base64_data):
    """Process frame from JavaScript"""
    global lip_reader, current_video_writer, video_frames, last_record_time
    
    try:
        # Only process frames when recording is active
        if not lip_reader.recording:
            return  # Do nothing if not recording
        
        # Decode frame
        image_bytes = base64.b64decode(base64_data)
        nparr = np.frombuffer(image_bytes, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Detect face and speaking (only when recording)
        lip_reader.detect_face_and_speaking(frame_rgb)
        
        # Record frames
        video_frames.append(cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2GRAY))
        last_record_time = time.time()
        
        # Update web interface status every few frames to avoid spam
        frame_counter = getattr(process_frame, 'counter', 0) + 1
        process_frame.counter = frame_counter
        
        if frame_counter % 5 == 1:  # Update UI every 5 frames
            js_code = f"""
            if (typeof updateStatus === 'function') {{
                updateStatus({str(lip_reader.recording).lower()}, 
                            {str(lip_reader.face_detected).lower()}, 
                            {str(lip_reader.speaking_detected).lower()});
            }}
            """
            display(Javascript(js_code))
            
        # Show recording progress
        if len(video_frames) % (lip_reader.fps * 2) == 0 and len(video_frames) > 0:  # Every 2 seconds
            duration = len(video_frames) / lip_reader.fps
            js_progress = f"""
            document.getElementById('result').innerHTML = 'üéôÔ∏è Recording... ({duration:.1f}s)';
            """
            display(Javascript(js_progress))
        
    except Exception as e:
        print(f"‚ùå Frame processing error: {e}")
        import traceback
        traceback.print_exc()

def toggle_recording():
    """Toggle recording state"""
    global lip_reader, video_frames, last_record_time
    
    was_recording = lip_reader.recording
    lip_reader.recording = not lip_reader.recording
    
    if lip_reader.recording:
        # Start recording
        video_frames = []
        last_record_time = time.time()
        
        # Reset face detection states
        lip_reader.face_detected = False
        lip_reader.speaking_detected = False
        lip_reader.prev_landmarks = None
        lip_reader.landmark_history = []
        lip_reader.debug_counter = 0
        
        print("üü¢ Recording started! Speak to the camera...")
        
        # Force UI update
        js_update = """
        if (typeof updateStatus === 'function') {
            updateStatus(true, false, false);
        }
        document.getElementById('result').innerHTML = 'üéôÔ∏è Recording... Speak to camera, press SPACE to stop and process';
        """
        display(Javascript(js_update))
        
        return "üü¢ RECORDING"
    else:
        # Stop recording and process
        if len(video_frames) >= lip_reader.fps * lip_reader.min_duration:
            process_recorded_video()
        else:
            print("‚ö†Ô∏è Recording too short, need at least 2 seconds")
        
        # Reset face detection states when stopping
        lip_reader.face_detected = False
        lip_reader.speaking_detected = False
        
        print("üî¥ Recording stopped!")
        
        # Force UI update
        js_update = """
        if (typeof updateStatus === 'function') {
            updateStatus(false, false, false);
        }
        document.getElementById('result').innerHTML = 'üîÑ Processing video...';
        """
        display(Javascript(js_update))
        
        return "üî¥ PAUSED"

def force_process():
    """Force process current buffer"""
    global video_frames
    
    if len(video_frames) > 0:
        print(f"‚ö° Force processing {len(video_frames)} frames...")
        process_recorded_video()
        return "‚ö° Processing..."
    else:
        return "‚ùå No frames to process"


def process_recorded_video():
    """Process recorded video frames with enhanced quality"""
    global video_frames, lip_reader
    
    try:
        if len(video_frames) < 20:
            print("‚ùå Not enough frames")
            return
        
        # Create temporary video file with enhanced quality
        output_path = tempfile.mktemp(suffix='.mp4')
        height, width = video_frames[0].shape
        
        # Use high-quality settings
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, lip_reader.fps, (width, height), False)
        
        # Enhanced preprocessing for better accuracy
        processed_frames = []
        for frame in video_frames:
            # Apply histogram equalization for better contrast
            enhanced = cv2.equalizeHist(frame)
            # Slight sharpening for better lip details  
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]], dtype=np.float32)
            sharpened = cv2.filter2D(enhanced, -1, kernel)
            processed_frames.append(sharpened)
            out.write(sharpened)
        
        out.release()
        
        print(f"üîÑ Processing final video ({len(video_frames)} frames)...")
        
        # Process video
        result = lip_reader.pipeline(output_path)
        
        # Clean up
        if os.path.exists(output_path):
            os.remove(output_path)
        
        if result and result.strip():
            clean_result = result.strip().upper()
            
            # Apply same filtering for final result
            words = clean_result.split()
            if len(words) > 0:
                most_common_word = max(set(words), key=words.count)
                repetition_ratio = words.count(most_common_word) / len(words)
                
                if repetition_ratio < 0.8:  # Slightly more lenient for final result
                    print(f"‚úÖ Lip reading result: '{clean_result}'")
                    
                    # Update web interface
                    js_code = f"""
                    document.getElementById('result').innerHTML = '‚úÖ Result: {clean_result}';
                    """
                    display(Javascript(js_code))
                else:
                    print(f"‚ö†Ô∏è Filtered repetitive result: '{clean_result}'")
                    js_code = """
                    document.getElementById('result').innerHTML = '‚ö†Ô∏è Result filtered (too repetitive)';
                    """
                    display(Javascript(js_code))
        else:
            print("‚ùå No clear speech detected")
            js_code = """
            document.getElementById('result').innerHTML = '‚ùå No clear speech detected';
            """
            display(Javascript(js_code))
        
        video_frames = []
        
    except Exception as e:
        print(f"‚ùå Video processing error: {e}")
        import traceback
        traceback.print_exc()

# Register callback functions
output.register_callback('process_frame', process_frame)
output.register_callback('toggle_recording', toggle_recording)
output.register_callback('force_process', force_process)

# Initialize and run
try:
    lip_reader = RealtimeLipReader()
    lip_reader.start_web_preview()
    
    # Set initial status (not recording)
    initial_status = f"""
    if (typeof updateStatus === 'function') {{
        updateStatus(false, false, false);
    }}
    """
    display(Javascript(initial_status))
    
    print("\nüéØ Ready! Workflow: SPACE to start ‚Üí Speak ‚Üí SPACE to stop & process")
    print("üì∑ Webcam preview visible above - results appear only after you stop recording")
    
except Exception as e:
    print(f"‚ùå Error: {e}")
    import traceback
    traceback.print_exc()